{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "from tokenizers import Tokenizer, AddedToken\n",
    "from transformers import PreTrainedTokenizerFast, AutoTokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "import sentencepiece as spm\n",
    "import regex as re\n",
    "from itertools import chain\n",
    "from copy import deepcopy\n",
    "import json\n",
    "from pprint import pprint\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define an example text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"\"\"OPENQASM 3.0;\n",
    "include \"stdgates.inc\";\n",
    "gate mcx _gate_q_0, _gate_q_1, _gate_q_2, _gate_q_3 {\n",
    "  h _gate_q_3;\n",
    "  p(pi/8) _gate_q_0;\n",
    "  p(pi/8) _gate_q_1;\n",
    "  p(pi/8) _gate_q_2;\n",
    "  p(pi/8) _gate_q_3;\n",
    "  cx _gate_q_0, _gate_q_1;\n",
    "  p(-pi/8) _gate_q_1;\n",
    "  cx _gate_q_0, _gate_q_1;\n",
    "  cx _gate_q_1, _gate_q_2;\n",
    "  p(-pi/8) _gate_q_2;\n",
    "  cx _gate_q_0, _gate_q_2;\n",
    "  p(pi/8) _gate_q_2;\n",
    "  cx _gate_q_1, _gate_q_2;\n",
    "  p(-pi/8) _gate_q_2;\n",
    "  cx _gate_q_0, _gate_q_2;\n",
    "  cx _gate_q_2, _gate_q_3;\n",
    "  p(-pi/8) _gate_q_3;\n",
    "  cx _gate_q_1, _gate_q_3;\n",
    "  p(pi/8) _gate_q_3;\n",
    "  cx _gate_q_2, _gate_q_3;\n",
    "  p(-pi/8) _gate_q_3;\n",
    "  cx _gate_q_0, _gate_q_3;\n",
    "  p(pi/8) _gate_q_3;\n",
    "  cx _gate_q_2, _gate_q_3;\n",
    "  p(-pi/8) _gate_q_3;\n",
    "  cx _gate_q_1, _gate_q_3;\n",
    "  p(pi/8) _gate_q_3;\n",
    "  cx _gate_q_2, _gate_q_3;\n",
    "  p(-pi/8) _gate_q_3;\n",
    "  cx _gate_q_0, _gate_q_3;\n",
    "  h _gate_q_3;\n",
    "}\n",
    "gate mcmt _gate_q_0, _gate_q_1, _gate_q_2, _gate_q_3 {\n",
    "  h _gate_q_3;\n",
    "  mcx _gate_q_0, _gate_q_1, _gate_q_2, _gate_q_3;\n",
    "  h _gate_q_3;\n",
    "}\n",
    "gate Oracle _gate_q_0, _gate_q_1, _gate_q_2, _gate_q_3 {\n",
    "  x _gate_q_0;\n",
    "  x _gate_q_1;\n",
    "  x _gate_q_2;\n",
    "  x _gate_q_3;\n",
    "  mcmt _gate_q_0, _gate_q_1, _gate_q_2, _gate_q_3;\n",
    "  x _gate_q_0;\n",
    "  x _gate_q_1;\n",
    "  x _gate_q_2;\n",
    "  x _gate_q_3;\n",
    "}\n",
    "gate Diffuser _gate_q_0, _gate_q_1, _gate_q_2, _gate_q_3 {\n",
    "  h _gate_q_0;\n",
    "  h _gate_q_1;\n",
    "  h _gate_q_2;\n",
    "  h _gate_q_3;\n",
    "  x _gate_q_0;\n",
    "  x _gate_q_1;\n",
    "  x _gate_q_2;\n",
    "  x _gate_q_3;\n",
    "  h _gate_q_3;\n",
    "  mcx _gate_q_0, _gate_q_1, _gate_q_2, _gate_q_3;\n",
    "  h _gate_q_3;\n",
    "  x _gate_q_0;\n",
    "  x _gate_q_1;\n",
    "  x _gate_q_2;\n",
    "  x _gate_q_3;\n",
    "  h _gate_q_0;\n",
    "  h _gate_q_1;\n",
    "  h _gate_q_2;\n",
    "  h _gate_q_3;\n",
    "}\n",
    "bit[4] c;\n",
    "qubit[4] q;\n",
    "h q[0];\n",
    "h q[1];\n",
    "h q[2];\n",
    "h q[3];\n",
    "Oracle q[0], q[1], q[2], q[3];\n",
    "Diffuser q[0], q[1], q[2], q[3];\n",
    "Oracle q[0], q[1], q[2], q[3];\n",
    "Diffuser q[0], q[1], q[2], q[3];\n",
    "Oracle q[0], q[1], q[2], q[3];\n",
    "Diffuser q[0], q[1], q[2], q[3];\n",
    "c[0] = measure q[0];\n",
    "c[1] = measure q[1];\n",
    "c[2] = measure q[2];\n",
    "c[3] = measure q[3];\n",
    "\"\"\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the Base Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/cm/model/Meta-Llama-3-8B-Instruct\")\n",
    "for text in texts:\n",
    "    tokens_encoded = tokenizer.encode(text)\n",
    "    tokens_decoded = []\n",
    "    for token in tokens_encoded:\n",
    "        tokens_decoded.append(tokenizer.decode(token))\n",
    "        \n",
    "    print('\\n--------------\\nNumber of tokens:', len(tokens_decoded))\n",
    "    print(tokens_decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.tokenize(\"_gate_q_14\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define and Test the rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _tokenize_line(command):\n",
    "    command = command.strip()\n",
    "    if not command:\n",
    "        return []\n",
    "\n",
    "    # gate\n",
    "    if command.startswith(\"gate\"):\n",
    "        gate_match = re.match(r\"gate\\s+(\\w+)(?:\\s*\\((.*?)\\))?\\s+([^{]+)\\s*{\", command)\n",
    "        if not gate_match:\n",
    "            raise SyntaxError(f\"Invalid gate definition: {command}\")\n",
    "        gate_name = gate_match.group(1)\n",
    "        params_part = gate_match.group(2) or \"\"\n",
    "        qubits_part = gate_match.group(3)\n",
    "        params = [p.strip() for p in params_part.split(\",\") if p.strip()]\n",
    "        qubits = [q.strip() for q in qubits_part.split(\",\") if q.strip()]\n",
    "        tokens = [\"gate\", gate_name] + params + qubits + [\"{\"]\n",
    "        # process the digit after the token\n",
    "        tokens = [re.sub(r'^(_gate_q_|unitary_|mcx_vchain_)\\d+$', r'\\1', t) for t in tokens]\n",
    "        return tokens\n",
    "\n",
    "    groups = re.match(r\"^(\\w+)(?:\\((.*?)\\))?\\s+([^;]+);\", command)\n",
    "    if groups:\n",
    "        op_name = groups.group(1)\n",
    "        params = groups.group(2)\n",
    "        targets = groups.group(3)\n",
    "        tokens = [op_name]\n",
    "        if params:\n",
    "            tokens += [\"(\"] + [p.strip() for p in params.split(\",\")] + [\")\"]\n",
    "        tokens += [t.strip() for t in targets.split(\",\")]\n",
    "        tokens = [token for token in tokens if token]\n",
    "        # process the digit after the token\n",
    "        tokens = [re.sub(r'^(_gate_q_|unitary_|mcx_vchain_)\\d+$', r'\\1', t) for t in tokens]\n",
    "        return tokens\n",
    "\n",
    "    if command == \"}\":\n",
    "        return [\"}\"]\n",
    "\n",
    "    raise SyntaxError(f\"Unrecognized command: {command}\")\n",
    "\n",
    "\n",
    "text = texts[0]\n",
    "lines = [line.strip() for line in text.split(\"\\n\") if line.strip()]\n",
    "total_tokens = []\n",
    "for line in lines:\n",
    "    try:\n",
    "        tokens = _tokenize_line(line)\n",
    "        total_tokens.extend(tokens)\n",
    "    except SyntaxError as e:\n",
    "        print(f\"Error in line: {line}\")\n",
    "        print(e)\n",
    "        continue\n",
    "print(total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_vocab = tokenizer.get_vocab()\n",
    "\n",
    "def load_json_to_list(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "# Use collected qasm files to collect new tokens\n",
    "file_path = 'inputs_list_all_0.json'\n",
    "data_list = load_json_to_list(file_path)\n",
    "\n",
    "new_vocab = []\n",
    "\n",
    "for i, data_item in enumerate(data_list):\n",
    "    lines = [line.strip() for line in data_item.split(\"\\n\") if line.strip()]\n",
    "    \n",
    "    total_tokens = []\n",
    "    for line in lines:\n",
    "        try:\n",
    "            tokens = _tokenize_line(line)\n",
    "            total_tokens.extend(tokens)\n",
    "        except SyntaxError as e:\n",
    "            # print(f\"Error in line: {line}\")\n",
    "            # print(e)\n",
    "            continue\n",
    "        \n",
    "    for token in total_tokens:\n",
    "        if token and token not in base_vocab and token not in new_vocab:\n",
    "            new_vocab.append(token)\n",
    "print(new_vocab)\n",
    "\n",
    "tokenizer.add_tokens(new_vocab)\n",
    "    \n",
    "tokenizer.save_pretrained('./Grover_Extend_Tokenizer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the New Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "for text in texts:\n",
    "    tokens_encoded = tokenizer.encode(text)\n",
    "    tokens_decoded = []\n",
    "    for idx, token in enumerate(tokens_encoded):\n",
    "        tokens_decoded.append(tokenizer.decode(token))\n",
    "        \n",
    "    print('\\n--------------\\nNumber of tokens:', len(tokens_decoded))\n",
    "    pprint(tokens_decoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now one can put the new tokenizer in the ~/model/Meta-Llama-3-8B-Instruct/., with the original tokenizer files being saved and moved to other places for additional usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLaMA-Factory",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
